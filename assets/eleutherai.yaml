---

- type: dataset
  name: The Pile
  # General
  organization: EleutherAI
  description: >
    A latge language model training dataset, used to train GPT-NeoX-20B.
  created_date: 2021-01-01
  url: https://arxiv.org/pdf/2101.00027.pdf
  datasheet: https://arxiv.org/pdf/2201.07311.pdf
  modality: text (English) and code
  size: 825 GB
  sample:
    - '...pot trending topics and the coverage around them. First up, there’s a
      bit of a visual redesign. Previously, clicking on a trending topic would highlight
      a story from one publication, and you’d have to scroll down past a live video
      section to view related stories. Facebook is replacing that system with a
      simple carousel, which does a better job of showing you different coverage
      options. To be clear, the change doesn’t affect how stories are sourced, according
      to Facebook. It’s still the same algorithm pickin...'
    - Total knee arthroplasty (TKA) is a promising treatment for endstage osteoarthritis
      (OA) of the knee for alleviating pain and restoring the function of the knee.
      Some of the cases with bilateral TKA are symptomatic, necessitating revision
      arthroplasty in both the knees. A bilateral revision TKA can be done ei
    - On the converse, the set-valued map $\Phi:[0,3]\rightrightarrows [0,3]$ $$\Phi(x):=\left\{\begin{array}{ll}
      \{1\} & \mbox{ if } 0\leq x<1\\ {}[1,2] & \mbox{ if } 1\leq x\leq 2\\ \{2\}
      &
    - This Court thus uses the same interpretation of V.R.C.P. 52(a) as it did *487
      under the previous statutory requirement found in 12 V.S.A. § 2385.  In essense,
      the defendants urge that this Court should reconsider the case of Green Mountain
      Marble Co. v. Highway Board, supra, and follow the Federal practice of looking
      to the evide
  analysis: >
    Analyses of the data's composition, document statistics,
    language/dialectal coverage, topical distribution, and biases are
    conducted are conducted in the paper
    [[The Pile Paper]](https://arxiv.org/pdf/2101.00027.pdf).
  # Construction
  dependencies: []
  license: >
    The Pile uses the MIT, allowing proprietary or open source use
    on the condition that the terms of the license as well as a copyright
    notice [[MIT]](https://arxiv.org/pdf/2201.07311.pdf).
  included: >
    The Pile data come from 22 sources, with over half of the data being from
    Common Crawl (Pile-CC; 227GB), fiction and nonfiction books (Books3; 101GB),
    biomedical articles (PubMed Central; 90GB), and code (Github; 95 GB).
    Refer to the paper for full decomposition
    [[Table 1]](https://arxiv.org/pdf/2101.00027.pdf#table.caption.2).
  excluded: >
    Authors report that they have excluded some datasets "because they were too
    small to be worth spending time or because the English component of the data
    did not merit inclusion on its own. Three datasets were excluded for other
    reasons: (1) US Congressional Records were excluded because it "reflects the
    opinions and biases of the political class over the past 200 years,
    including segregationism and xenophobia." (2) Online Fanfiction resources
    amounting to Hundreds of GiB were excluded on logistical grounds.
    (3) Literotica, platform where users can upload short-form erotic fiction,
    was excluded because the authors decided to exclude fanfiction, the
    corpus would require significant investigation, and corpus contain
    significant amount of stereotyping
    [[Appendix B]](https://arxiv.org/pdf/2101.00027.pdf).
  quality_control: >
    In addition to the data inclusion and exclusion decisions, the quality was
    controlled through filtering for English (pycld2 language classifier),
    filtering for documents similar to OpenWebText2 (classifier on CommonCrawl),
    and several forms of deduplication as detailed in the paper
    [[Appendix C]](https://arxiv.org/pdf/2101.00027.pdf#appendix.1.C)
    [[Appendix D]](https://arxiv.org/pdf/2101.00027.pdf#appendix.1.D).
  # Downstream
  access:
    value: open
    explanation: >
      The dataset is freely available to the public and
      can be downloaded from The Eye
      [[The Pile]](https://mystic.the-eye.eu/public/AI/pile/).
  intended_uses: >
    The Pile was intended to be used as a high quality large text dataset for
    language modeling tasks, explained in more detail in the paper
    [[Section 1]](https://arxiv.org/pdf/2101.00027.pdf#section.1).
  prohibited_uses: none
  monitoring: none
  feedback: >
    Feedback can be given by emailing the authors at contact at eleuther.ai.

- type: model
  name: GPT-J
  organization: EleutherAI
  description: GPT-J is an open-source autoregressive language model.
  created_date:
    value: 2021-06-04
    explanation: Date model blog post was published
  url: https://arankomatsuzaki.wordpress.com/2021/06/04/gpt-j/
  model_card: ''
  modality: text (English)
  analysis: ''
  size: 6B parameters (dense model)
  dependencies: [The Pile]
  training_emissions: ''
  training_time: ''
  training_hardware: 'TRC (Unspecified # of TPU v3-8s)'
  quality_control: ''
  access:
    value: open
    explanation: >
      The model can be downloaded for free from [[The Eye]](https://mystic.the-eye.eu/public/AI/GPT-J-6B/step_383500.tar.zstd)
  license:
    value: Apache 2.0
    explanation: >
      As indicated in the [[Github repository]](https://github.com/kingoflolz/mesh-transformer-jax)
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: GPT-Neo
  organization: EleutherAI
  description: ''
  created_date:
    value: 2021-03-21
    explanation: Date Github repo was update
  url: https://github.com/EleutherAI/gpt-neo
  model_card: ''
  modality: Text (English)
  analysis: ''
  size: 2.7B parameters (dense model)
  dependencies: [The Pile]
  training_emissions: ''
  training_time: ''
  training_hardware: ''
  quality_control: ''
  access:
    value: open
    explanation: >
      The model can be downloaded for free from [[The Eye]](https://mystic.the-eye.eu/public/AI/gptneo-release/GPT3_2-7B/)
  license:
    value: MIT
    explanation: ''
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: ''

- type: model
  name: GPT-NeoX
  # General
  organization: EleutherAI
  description: >
    GPT-NeoX (20B) is an open-sourced autoregressive language model.
  created_date: 2022-02-02
  url: http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf
  model_card: https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/20B_model_card.md
  modality: text (English) and code
  size: 20B parameters (dense model)
  analysis: >
    The model was evaluated on standard NLP benchmarks: LAMBADA, ANLI,
    HellaSwag, MMLU among others
    [[Section 4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#section.4).
  # Construction
  dependencies: [The Pile]
  training_emissions:
    value: 31.73 tCO2e
    explanation: >
      The amount of emission during the development and training of
      the model based on the author's estimation
      [[Section 6.4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.6.4).
  training_time:
    value: 47.10 petaflop/s-day
    explanation: >
      Training time was reported as 1830 hours reported by the authors, equaling
      76.25 days.
      [[Section 6.4]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.6.4).
      The authors report that 96 (12 * 8) A100 GPUs were used during the
      training.
      The A100 GPUs have a single precision performance of 0.0195 petaflops
      [[A100 Datasheet]](https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/nvidia-a100-datasheet-us-nvidia-1758950-r4-web.pdf).
      Assuming the estimated utilization is 33%, following
      [[OpenAI AI and Computer Blog]](https://openai.com/blog/ai-and-compute/#addendum),
      the training time is 47.10 petaflop/s-day (76.25 * 96 * 0.0195 * 0.33).
  training_hardware:
    value: 12 x 8 A100 GPUs
    explanation: >
      As outline by the authors
      [[Section 2.3]](http://eaidata.bmk.sh/data/GPT_NeoX_20B.pdf#subsection.2.3)
  quality_control: none
  # Downstream
  access:
    value: open
    explanation: >
      The model can be downloaded for free The Eye
      [[GPT-NeoX-20B]](https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/).
  license:
    value: Apache 2.0
    explanation: >
      As indicated in the accompanying blog post
      [[EleutherAI Blog Post]](https://blog.eleuther.ai/announcing-20b/).
  intended_uses: >
    As stated in the model card: "GPT-NeoX-20B learns an inner representation
    of the English language that can be used to extract features useful for
    downstream tasks. The model is best at what it was pretrained for however,
    which is generating text from a prompt.
    Due to the generality of the pretraining set, it has acquired the ability
    to generate completions across a wide range of tasks - from programming to
    fiction writing
    [[Model Card]](https://mystic.the-eye.eu/public/AI/models/GPT-NeoX-20B/20B_model_card.md)."
  prohibited_uses: none
  monitoring: none
  feedback: >
    Feedback can be provided using the  # 20b channel in EleutherAI Discord
    group
    [[EleutherAI Blog Post]](https://blog.eleuther.ai/announcing-20b/).
    Find the Discord link in the FAQ page
    [[FAQ]](https://www.eleuther.ai/faq/).

- type: application
  name: GooseAI API
  # General
  organization: GooseAI
  description: >
    GooseAI API is an API service providing access to NLP services.
  created_date: unknown
  url: goose.ai
  # Construction
  dependencies: [GPT-NeoX]
  adaptation: unknown
  output_space:
    value: Text Generation, Text Completion
    explanation: >
      Question/Answer and Classification tasks are coming soon according to
      GooseAI
      [[Main Page]](goose.ai).
  quality_control: unknown
  # Downstream
  access:
    value: limited
    explanation: >
      GooseAI API can be accessed by signing up on the goose.ai website.
  license:
    value: Limited use license
    explanation: >
      "Subject to Customer’s strict compliance with this TOS, GooseAI grants
      Customer a limited, non-exclusive, non-transferable, non-sublicensable,
      revocable license to access and use the Platform as described in and
      subject to this TOS"
      [[GooseAI Terms of Service]](https://goose.ai/docs/tos)
  terms_of_service: https://goose.ai/docs/tos
  intended_uses: >
    Intended to be used as an NLP infrastructure.
  prohibited_uses:
    value: >
      Illegal or abusive activity, security violations, network abuse
    explanation: >
      Prohibited uses are detailed in the Acceptable Use Policy
      [[GooseAI Acceptable Use Policy]](https://goose.ai/docs/aup).
  monitoring:
    value: At will monitoring by the provider
    explanation: >
      In the "GooseAI Monitoring and Enforcement" section of GooseAI's
      Acceptable Use Policy (AUP), it is stated that Goose.AI has the right to
      investigate any suspected violation of its AUP
      [[GooseAI Acceptable Use Policy]](https://goose.ai/docs/aup).
  feedback:
    value: Email support
    explanation: >
      In the "Error Reporting and Feedback" section of the Goose.ai Terms of
      Service, GooseAI asks all the feedback to be sent to support at goose.ai
      [[GooseAI Terms of Service]](https://goose.ai/docs/tos).
  # Deployment
  monthly_active_users: unknown
  user_distribution: unknown
  failures: unknown

- type: model
  name: Pythia
  # General
  organization: EleutherAI
  description: >
    The Pythia Scaling Suite is a collection of models developed to facilitate interpretability
    research. It contains two sets of eight models of sizes 70M, 160M, 410M, 1B,
    1.4B, 2.8B, 6.9B, and 12B. For each size, there are two models: one trained
    on the Pile, and one trained on the Pile after the dataset has been globally
    deduplicated. All 8 model sizes are trained on the exact same data, in the exact
    same order. 154 intermediate checkpoints are provided per model.
  created_date:
    value: 2023-04-03
    explanation: The date the [[model paper]](https://arxiv.org/abs/2304.01373)
      was released.
  url: https://arxiv.org/pdf/2304.01373.pdf
  model_card: ''
  modality: text (English)
  size:
    value: 12B parameters (dense model)
    explanation: Size of the largest model in the Pythia family.
  analysis: ''
  # Construction
  dependencies: [The Pile]
  training_emissions: unknown
  training_time:
    value: 72,300 total GPU-hours
    explanation: Training time for the Pythia-12B model. See [[Table 6 - Training
      Hardware and GPU Hours]](https://arxiv.org/pdf/2304.01373.pdf#page=20).
  training_hardware:
    value: 256 40GB A100 GPUs
    explanation: No. of GPUs used to train the Pythia-12B model. See [[Table 6 -
      Training Hardware and GPU Hours]](https://arxiv.org/pdf/2304.01373.pdf#page=20).
  quality_control: ''
  # Downstream
  access:
    value: open
    explanation: >
      All Pythia models can be found on their [[Hugging Face page]](https://huggingface.co/EleutherAI/pythia-12b).
      Trained models, analysis code, training code, and training data can be found
      on the [[Pythia GitHub page]](https://github.com/EleutherAI/pythia).
  license:
    value: Apache 2.0
    explanation: See License section of the [[Pythia GitHub page]](https://github.com/EleutherAI/pythia#license).
  intended_uses:
    value: |
      - The primary intended use of Pythia is **research** on the behavior, functionality, and limitations of large language models. This suite is intended to provide a controlled setting for performing scientific experiments. We also provide 154 checkpoints per model: initial step0, 10 log-spaced checkpoints step{1,2,4...512}, and 143 evenly-spaced checkpoints from step1000 to step143000. These checkpoints are hosted on Hugging Face as branches. Note that branch 143000 corresponds exactly to the model checkpoint on the main branch of each model.
      - You may also further fine-tune and adapt Pythia-12B for deployment, as long as your use is in accordance with the Apache 2.0 license. Pythia models work with the Hugging Face Transformers Library. If you decide to use pre-trained Pythia-12B as a basis for your fine-tuned model, please conduct your own risk and bias assessment.
    explanation: See [[Uses and Limitations]](https://huggingface.co/EleutherAI/pythia-12b#uses-and-limitations)
  prohibited_uses:
    value: |
      Out-of-scope uses:
        - The Pythia Suite is not intended for deployment. It is not a in itself a product and cannot be used for human-facing interactions. For example, the model may generate harmful or offensive text. Please evaluate the risks associated with your particular use case.
        - Pythia models are English-language only, and are not suitable for translation or generating text in other languages.
        - Pythia-12B has not been fine-tuned for downstream contexts in which language models are commonly deployed, such as writing genre prose, or commercial chatbots. This means Pythia-12B will not respond to a given prompt the way a product like ChatGPT does. This is because, unlike this model, ChatGPT was fine-tuned using methods such as Reinforcement Learning from Human Feedback (RLHF) to better “follow” human instructions.

      Limitations and biases:
        - The core functionality of a large language model is to take a string of text and predict the next token. The token used by the model need not produce the most “accurate” text. Never rely on Pythia-12B to produce factually accurate output.
        - This model was trained on the Pile, a dataset known to contain profanity and texts that are lewd or otherwise offensive. See Section 6 of the Pile paper for a discussion of documented biases with regards to gender, religion, and race. Pythia-12B may produce socially unacceptable or undesirable text, even if the prompt itself does not include anything explicitly offensive.
        - If you plan on using text generated through, for example, the Hosted Inference API, we recommend having a human curate the outputs of this language model before presenting it to other people. Please inform your audience that the text was generated by Pythia-12B.
    explanation: See [[Uses and Limitations]](https://huggingface.co/EleutherAI/pythia-12b#uses-and-limitations)
  monitoring: ''
  feedback: https://github.com/EleutherAI/pythia/issues
