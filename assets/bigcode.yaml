---

- type: dataset
  name: The Stack
  # General
  organization: BigCode
  description: >
    The Stack contains over 6TB of permissively-licensed source code files covering
    358 programming languages. The dataset was created as part of the BigCode Project,
    an open scientific collaboration working on the responsible development of Large
    Language Models for Code (Code LLMs). The Stack serves as a pre-training dataset
    for Code LLMs, i.e., code-generating AI systems which enable the synthesis of
    programs from natural language descriptions as well as other from code snippets.
  created_date:
    value: 2022-10-27
    explanation: The date the dataset was announced in a [[BigCode twitter post]](https://twitter.com/BigCodeProject/status/1585631176353796097).
  url: https://arxiv.org/pdf/2211.15533.pdf
  datasheet: https://huggingface.co/datasets/bigcode/the-stack
  modality: code (358 programming languages)
  size: 6.4TB
  sample: []
  analysis: ''
  # Construction
  dependencies: []
  license:
    value: >
      "The Stack is a collection of source code from repositories with various licenses.
      Any use of code gathered in The Stack must abide by the codeâ€™s original license
      terms, including attribution clauses when relevant."
    explanation: See [[What is the license for The Stack dataset?]](https://www.bigcode-project.org/docs/about/the-stack/#licenses)
  included: ''
  excluded: ''
  quality_control:
    value: The authors provide a tool called [[Am I in The Stack]](https://huggingface.co/spaces/bigcode/in-the-stack)
      for developers to search The Stack for copies of their code, and provide a
      process for code to be removed from the dataset by following the instructions
      provided [[here]](https://www.bigcode-project.org/docs/about/the-stack/#how-can-i-request-that-my-data-be-removed-from-the-stack).
    explanation: As mentioned on [[The Stack website]](https://www.bigcode-project.org/docs/about/the-stack/).
  # Downstream
  access:
    value: open
    explanation: The dataset can be downloaded from [[Hugging Face]](https://huggingface.co/datasets/bigcode/the-stack).
      The code used to build the dataset along with data preprocessing code is available
      on the [[GitHub repository]](https://github.com/bigcode-project/bigcode-dataset).
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: Feedback can be provided on [[GitHub Issues]](https://github.com/bigcode-project/bigcode-dataset/issues)
    or can be emailed at contact at bigcode-project.org.

- type: model
  name: SantaCoder
  # General
  organization: BigCode
  description: >
    The SantaCoder models are a series of 1.1B parameter models trained on the Python,
    Java, and JavaScript subset of The Stack (v1.1).
  created_date:
    value: 2023-01-09
    explanation: The date the [[paper]](https://arxiv.org/abs/2301.03988) was released.
  url: https://arxiv.org/pdf/2301.03988.pdf
  model_card: ''
  modality: code (Python, Java, JavaScript)
  size: 1.1B parameters
  analysis: ''
  # Construction
  dependencies: [The Stack]
  training_emissions: ''
  training_time:
    value: 6.2 days
    explanation: See [[Hardware]](https://huggingface.co/bigcode/santacoder#hardware)
  training_hardware:
    value: 96 Tesla V100
    explanation: See [[Hardware]](https://huggingface.co/bigcode/santacoder#hardware)
  quality_control: ''
  # Downstream
  access:
    value: open
    explanation: The model weights can be downloaded from [[Hugging Face]](https://huggingface.co/bigcode/santacoder).
  license:
    value: CodeML Open RAIL-M v0.1 license
    explanation: The full license can be found [[here]](https://huggingface.co/spaces/bigcode/license).
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: Feedback can be emailed at contact at bigcode-project.org.

- type: model
  name: StarCoder
  # General
  organization: BigCode
  description: >
    StarCoder is a 15B LLM trained for 1T tokens on source code and natural language
    text, with context window of 8k tokens. Its training data incorporates more
    that 80 different programming languages from The Stack (v1.2), as well as text
    extracted from github issues and commits and from notebooks.
  created_date:
    value: 2023-05-04
    explanation: >
      The date the model was announced in a [[BigCode twitter post]](https://twitter.com/BigCodeProject/status/1654174941976068119).
  url: ''
  model_card: ''
  modality: text (English), code (80+ programming languages)
  size: 15.5B parameters
  analysis: ''
  # Construction
  dependencies: [The Stack]
  training_emissions: ''
  training_time:
    value: 24 days
    explanation: See [[Hardware]](https://huggingface.co/bigcode/starcoder#hardware)
  training_hardware:
    value: 512 Tesla A100
    explanation: See [[Hardware]](https://huggingface.co/bigcode/starcoder#hardware)
  quality_control: ''
  # Downstream
  access:
    value: open
    explanation: The model weights can be downloaded from [[Hugging Face]](https://huggingface.co/bigcode/starcoder).
      See [[GitHub repository]](https://github.com/bigcode-project/starcoder) for
      code.
  license:
    value: BigCode OpenRAIL-M v1 license agreement.
    explanation: The full agreement can be found [[here]](https://huggingface.co/spaces/bigcode/bigcode-model-license-agreement).
  intended_uses: ''
  prohibited_uses: ''
  monitoring: ''
  feedback: Feedback can be emailed at contact at bigcode-project.org.
