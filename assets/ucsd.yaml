---

- type: model
  name: Baize
  # General
  organization: UC San Diego, Sun Yat-sen University
  description: >
    Baize is an open-source chat model that uses parameter-efficient tuning (specifically,
    LoRA) to enhance the LLaMA model. It uses 100k dialogs generated by letting
    ChatGPT chat with itself. It also uses Alpaca's data to improve its performance.
  created_date:
    value: 2023-04-03
    explanation: >
      The date the [[paper]](https://arxiv.org/abs/2304.01196) was released.
  url: https://arxiv.org/pdf/2304.01196.pdf
  model_card: ''
  modality: text
  size:
    value: 30B parameters (dense model)
    explanation: Available in 7B, 13B and 30B sizes.
  analysis: ''
  # Construction
  dependencies:
    - LLaMa
    - Alpaca dataset
    - Baize Quora dialogs
    - Baize StackOverflow dialogs
  training_emissions:
    value: 3.33 kg CO2 eq.
    explanation: >
      We estimate to have emitted 0.83, 1.48, 3.33 and 0.46 kg CO2 eq. for training
      Baize-7B, 13B, 30B and healthcare models, respectively. The total carbon footprint
      is equal to 0.004% of pretraining the corresponding LLaMA models. See [[Limitations,
      Risks and Environmental Impact]](https://arxiv.org/pdf/2304.01196.pdf#section.9).
  training_time:
    value: 36 hours
    explanation: Time taken to train the 30B model. See [[Table 3]](https://arxiv.org/pdf/2304.01196.pdf#page=4)
  training_hardware: A single NVIDIA A100-80GB GPU
  quality_control: ''
  # Downstream
  access:
    value: open
    explanation: The model weights, data and code can be found in the [[GitHub repository]](https://github.com/project-baize/baize-chatbot).
      Model weights can be loaded from [[Hugging Face]](https://huggingface.co/project-baize).
  license: CC-BY-NC 4.0
  intended_uses: All model weights and data are for **research** use ONLY. Commercial
    use is strictly prohibited.
  prohibited_uses: ''
  monitoring: ''
  feedback: ''
